

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Siteng Huang 黄思腾 - Siteng Huang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Siteng Huang">
<meta property="og:title" content="Siteng Huang 黄思腾">


  <link rel="canonical" href="http://localhost:4000/">
  <meta property="og:url" content="http://localhost:4000/">



  <meta property="og:description" content="Siteng Huang 黄思腾">





  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Siteng Huang",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Siteng Huang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Siteng Huang</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#news">News</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#publications">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#experience">Experience</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#services">Services</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#misc">Misc</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.png" class="author__avatar" alt="Siteng Huang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Siteng Huang</h3>
    <p class="author__bio">Ph.D. Student @ Zhejiang University</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Hangzhou, China</li>
      
      
      
      
        <li><a href="mailto:siteng.huang@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/KyonHuang"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/bighuang624"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mhpkWSYAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-9735-1186"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Siteng Huang 黄思腾">
    <meta itemprop="description" content="Siteng Huang 黄思腾">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Siteng Huang 黄思腾
</h1>
          
        
        
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <!-- Hi! I am Siteng Huang (黄思腾 in Chinese). I am a joint Ph.D. student of [Zhejiang University](http://www.zju.edu.cn/) and [Westlake University](https://www.westlake.edu.cn/), advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/about/faculty/201912/t20191206_2513.shtml). And I am a member of [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/) in Westlake University, and currently also a research intern at <a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a>. Prior to my Ph.D. career, I received my B.Eng. Degree from School of Computer Science, [Wuhan University](https://www.whu.edu.cn/) in 2019. -->
<p>Hi! I am Siteng Huang (黄思腾 in Chinese). I am a Ph.D. student at <a href="http://www.zju.edu.cn/">Zhejiang University</a>, advised by Prof. <a href="https://en.westlake.edu.cn/about/faculty/201912/t20191206_2513.shtml">Donglin Wang</a>. Additionally, I am involved in a joint program with <a href="https://www.westlake.edu.cn/">Westlake University</a> and am a member of <a href="https://milab.westlake.edu.cn/">Machine Intelligence Laboratory (MiLAB)</a>. Currently, I am also a research intern at <a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a>. Prior to my Ph.D. career, I received my B.Eng. Degree from School of Computer Science, <a href="https://www.whu.edu.cn/">Wuhan University</a> in 2019.</p>

<p><span style="color:red;">I am <b>seeking exciting industry opportunities for both research and applications after Ph.D. graduation (June 2024)</b>. Please feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a> if you are interested!</span></p>

<h2 id="research-interests">Research Interests</h2>

<!-- I am interested in technologies that allow machines and robots to learn like humans. In particular, I am committed to giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms. Currently, my areas of interest include meta-learning, multi-task learning, and transfer learning on few/zero-shot learning tasks. I am also interested in deep learning, computer vision, and multimodal machine learning. -->

<p>Currently, my research has centered on</p>

<ul>
  <li><strong>AIGC</strong>: text-to-image generation (T2I), customized &amp; controllable generation</li>
  <li><strong>Multi-modal large models</strong>: parameter-efficient fine-tuning (PEFT / PETL), vision-language pre-trained models (VLM), text-video retrieval (TVR)</li>
  <li><strong>Data-efficient learning</strong>: few-shot learning (FSL), compositional zero-shot learning (CZSL), meta-learning</li>
</ul>

<p>I am always looking for related colaboration. If you are interested to chat with me, feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a>.</p>

<!-- language-augmented vision -->

<!-- In the longer term, I am more concerned about

* giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms. -->

<!-- 
1. 快速迁移，尤其是大模型
2. 机器人的主动学习，感知智能与行为智能 embodied
3. 开放世界
-->

<!-- I am interested in technologies that allow machines and robots to learn like humans. In particular, I am committed to giving robots the ability to understand the world and learn from previous experiences, so that they can complete new tasks, acquire new skills or adapt to new environments rapidly with fewer samples through learning algorithms.  -->

<h2 id="news">News</h2>

<ul>
  <li><strong>[Semptember 6, 2023]</strong> A new <a href="https://arxiv.org/abs/2309.01141">paper</a> on diffusion model-based zero-shot visual grounding was released.</li>
  <li><strong>[July 24, 2023]</strong> <a href="https://scholar.googleblog.com/2023/07/2023-scholar-metrics-released.html">2023 Scholar Metrics</a> was released by Google Scholar. Our paper “<a href="https://kyonhuang.top/publication/dual-self-attention-network">DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting</a>” ranked <strong><a href="https://scholar.google.com/citations?hl=zh-CN&amp;oe=GB&amp;view_op=list_hcore&amp;venue=V-IMg2OTpU8J.2023&amp;vq=eng_databasesinformationsystems&amp;cstart=20">26th</a></strong> of the CIKM conference according to the citations within five years, and <strong>8th</strong> among papers published in the same year.</li>
  <li><strong>[April 2, 2023]</strong> <a href="https://kyonhuang.top/publication/reference-limited-CZSL">One paper about reference-limited compositional learning</a> got accepted for ICMR 2023. Congratulations to all collaborators!</li>
  <li><strong>[February 28, 2023]</strong> <a href="https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning">One paper about parameter-efficient text-video retrieval</a> got accepted for CVPR 2023. Congratulations to all collaborators!</li>
  <li><strong>[July 4, 2022]</strong> One paper got accepted for ECCV 2022.</li>
  <li><strong>[March 14, 2022]</strong> Started as a research intern at DAMO Academy, Alibaba Group.</li>
</ul>

<!-- **Service**: Always open to paper review, talk and organizing opportunities. Feel free to reach out to me if you are interested. -->
<!-- {: .notice--info} -->

<!-- Always open to research interns, cooperation and review opportunities. Feel free to reach out to me if you are interested. My email address is `huangsiteng [at] westlake.edu.cn`.
{: .notice--info} -->

<!-- **Hiring**: We are looking for **postdoctors, research assistants and visiting students for MiLAB in Westlake University** (currently only for Chinese). More information about requirements can be found [here](https://milab.westlake.edu.cn/contact.html), and if you are still in school, being a visiting student is also welcome. Please send email to `mi_lab[AT]westlake.edu.cn` with your CV if you are interested. Specially, if you are interested in my research direction and would like to be my collaborator after coming, please specify in the email and also send a copy to me.
{: .notice--info} -->

<h2 id="publications">Publications</h2>

<p>†: Equal contribution</p>

<h3 id="peer-reviewed-conference">Peer-reviewed Conference</h3>

<!-- <a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> **Siteng Huang**, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang, &quot;VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval&quot;. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023* (**CVPR 2023**). [[project page](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)] [[arXiv](https://arxiv.org/abs/2211.12764)] [[github](https://github.com/bighuang624/VoP)] [[ModelScope](https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary)]

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> **Siteng Huang**, Qiyao Wei, Donglin Wang, &quot;[Reference-Limited Compositional Zero-Shot Learning](https://doi.org/10.1145/3591106.3592225)&quot;. In *Proceedings of the 2023 ACM International Conference on Multimedia Retrieval* (**ICMR 2023**). [[project page](https://kyonhuang.top/publication/reference-limited-CZSL)] [[arXiv](https://arxiv.org/abs/2208.10046)] [[github](https://github.com/bighuang624/RL-CZSL)] 

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> Min Zhang, **Siteng Huang**, Wenbin Li, Donglin Wang, &quot;[Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation](https://link.springer.com/chapter/10.1007/978-3-031-20044-1_26)&quot;. In *Proceedings of the European Conference on Computer Vision 2022* (**ECCV 2022**). [[arXiv](https://arxiv.org/abs/2207.06989)] [[Chinese intro](https://zhuanlan.zhihu.com/p/543878686)] [[github](https://github.com/remiMZ/HTS-ECCV22)]

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> Min Zhang, **Siteng Huang**, Donglin Wang, &quot;[Domain Generalized Few-shot Image Classification via Meta Regularization Network](https://ieeexplore.ieee.org/abstract/document/9747620)&quot;. In *Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing* (**ICASSP 2022**). [[pdf](https://kyonhuang.top/files/MRN/ICASSP22-MRN.pdf)] [[github](https://github.com/remiMZ/MRN-ICASSP22)]

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> Zifeng Zhuang, Xintao Xiang, **Siteng Huang**, Donglin Wang, &quot;[HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network](https://dl.acm.org/doi/10.1145/3460426.3463614)&quot;. In *Proceedings of the 2021 ACM International Conference on Multimedia Retrieval* (**ICMR 2021**). [[pdf](https://kyonhuang.top/files/HINFShot/ICMR21-HINFShot.pdf)]

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> Zhengyu Chen, Jixie Ge, Heshen Zhan, **Siteng Huang**, Donglin Wang, &quot;[Pareto Self-Supervised Training for Few-Shot Learning](https://ieeexplore.ieee.org/document/9577454)&quot;. In *Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition* (**CVPR 2021**). [[arXiv](https://arxiv.org/abs/2104.07841)] [[open access](https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.html)] 

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> **Siteng Huang**, Min Zhang, Yachen Kang, Donglin Wang, &quot;[Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/16957)&quot;. In *Proceedings of the 35th AAAI Conference on Artificial Intelligence* (**AAAI 2021**). [[project page](https://kyonhuang.top/publication/attributes-guided-attention-module)] [[arXiv](https://arxiv.org/abs/2009.04724)] [[bib](https://kyonhuang.top/publication/attributes-guided-attention-module#bibtex)] [[code](https://github.com/bighuang624/AGAM)] [[poster](https://kyonhuang.top/files/AGAM/aaai21-AGAM-poster.pdf)] [[slide](https://kyonhuang.top/files/AGAM/aaai21-AGAM-presentation.pdf)]

<a href="https://arxiv.org/abs/2303.15230"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat"></a> **Siteng Huang**, Donglin Wang, Xuehan Wu, Ao Tang, &quot;[DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting](https://dl.acm.org/doi/abs/10.1145/3357384.3358132)&quot;. In *Proceedings of the 28th ACM International Conference on Information and Knowledge Management* (**CIKM 2019**). [[project page](https://kyonhuang.top/publication/dual-self-attention-network)] [[pdf](https://kyonhuang.top/files/DSANet/Huang-DSANet.pdf)] [[bib](https://kyonhuang.top/publication/dual-self-attention-network#bibtex)] [[code](https://github.com/bighuang624/DSANet)] [[poster](https://kyonhuang.top/files/DSANet/cikm19-DSANet-poster.pdf)] [[slide](https://kyonhuang.top/files/DSANet/cikm19-DSANet-presentation.pdf)] -->

<p><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.html" target="_blank"><img src="https://img.shields.io/badge/CVPR-2023-blue?style=flat-square" /></a> <u>Siteng Huang</u>, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, Donglin Wang, "<strong>VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval</strong>". In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023</em>. [<a href="https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning">project page</a>] [<a href="https://arxiv.org/abs/2211.12764">arXiv</a>] [<a href="https://www.youtube.com/watch?v=ymdkiSSuOmI">video (Youtube)</a>] [<a href="https://github.com/bighuang624/VoP">github</a>] [<a href="https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary">ModelScope</a>] [<a href="https://kyonhuang.top/files/VoP/CVPR23-VoP-poster.pdf">poster</a>] [<a href="https://kyonhuang.top/files/VoP/CVPR23-VoP-presentation.pdf">slide</a>] <a href="https://github.com/bighuang624/VoP" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/VoP?style=social" /></a></p>

<p><a href="https://doi.org/10.1145/3591106.3592225" target="_blank"><img src="https://img.shields.io/badge/ICMR-2023-blue?style=flat-square" /></a> <u>Siteng Huang</u>, Qiyao Wei, Donglin Wang, "<strong>Reference-Limited Compositional Zero-Shot Learning</strong>". In <em>Proceedings of the 2023 ACM International Conference on Multimedia Retrieval</em>. [<a href="https://kyonhuang.top/publication/reference-limited-CZSL">project page</a>] [<a href="https://arxiv.org/abs/2208.10046">arXiv</a>] [<a href="https://drive.google.com/file/d/1_wE_zbyvuGil_LrkmumotkRTLJJEUfCm/view?usp=drive_link">video (Google Drive)</a>] [<a href="https://github.com/bighuang624/RL-CZSL">github</a>] [<a href="https://kyonhuang.top/files/RLCZSL/ICMR23-RLCZSL-presentation.pdf">slide</a>]</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-20044-1_26" target="_blank"><img src="https://img.shields.io/badge/ECCV-2022-blue?style=flat-square" /></a> Min Zhang, <u>Siteng Huang</u>, Wenbin Li, Donglin Wang, "<strong>Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation</strong>". In <em>Proceedings of the European Conference on Computer Vision 2022</em>. [<a href="https://arxiv.org/abs/2207.06989">arXiv</a>] [<a href="https://zhuanlan.zhihu.com/p/543878686">Chinese intro</a>] [<a href="https://github.com/remiMZ/HTS-ECCV22">github</a>] <a href="https://scholar.google.com/citations?view_op=view_citation&amp;citation_for_view=mhpkWSYAAAAJ:Tyk-4Ss8FVUC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&amp;query=publications.3.citations&amp;url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&amp;logo=googlescholar&amp;style=social" /></a></p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/9747620" target="_blank"><img src="https://img.shields.io/badge/ICASSP-2022-blue?style=flat-square" /></a> Min Zhang, <u>Siteng Huang</u>, Donglin Wang, "<strong>Domain Generalized Few-shot Image Classification via Meta Regularization Network</strong>". In <em>Proceedings of the 2022 IEEE International Conference on Acoustics, Speech and Signal Processing</em>. [<a href="https://kyonhuang.top/files/MRN/ICASSP22-MRN.pdf">pdf</a>] [<a href="https://github.com/remiMZ/MRN-ICASSP22">github</a>]</p>

<p><a href="https://dl.acm.org/doi/10.1145/3460426.3463614" target="_blank"><img src="https://img.shields.io/badge/ICMR-2021-blue?style=flat-square" /></a> Zifeng Zhuang, Xintao Xiang, <u>Siteng Huang</u>, Donglin Wang, "<strong>HINFShot: A Challenge Dataset for Few-Shot Node Classification in Heterogeneous Information Network</strong>". In <em>Proceedings of the 2021 ACM International Conference on Multimedia Retrieval</em>. [<a href="https://kyonhuang.top/files/HINFShot/ICMR21-HINFShot.pdf">pdf</a>]</p>

<p><a href="https://ieeexplore.ieee.org/document/9577454" target="_blank"><img src="https://img.shields.io/badge/CVPR-2021-blue?style=flat-square" /></a> Zhengyu Chen, Jixie Ge, Heshen Zhan, <u>Siteng Huang</u>, Donglin Wang, "<strong>Pareto Self-Supervised Training for Few-Shot Learning</strong>". In <em>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. [<a href="https://arxiv.org/abs/2104.07841">arXiv</a>] [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pareto_Self-Supervised_Training_for_Few-Shot_Learning_CVPR_2021_paper.html">open access</a>] <a href="https://scholar.google.com/citations?view_op=view_citation&amp;citation_for_view=mhpkWSYAAAAJ:2osOgNQ5qMEC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&amp;query=publications.1.citations&amp;url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&amp;logo=googlescholar&amp;style=social" /></a></p>

<p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16957" target="_blank"><img src="https://img.shields.io/badge/AAAI-2021-blue?style=flat-square" /></a> <u>Siteng Huang</u>, Min Zhang, Yachen Kang, Donglin Wang, "<strong>Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition</strong>". In <em>Proceedings of the 35th AAAI Conference on Artificial Intelligence</em>. [<a href="https://kyonhuang.top/publication/attributes-guided-attention-module">project page</a>] [<a href="https://arxiv.org/abs/2009.04724">arXiv</a>] [<a href="https://github.com/bighuang624/AGAM">code</a>] [<a href="https://kyonhuang.top/files/AGAM/aaai21-AGAM-poster.pdf">poster</a>] [<a href="https://kyonhuang.top/files/AGAM/aaai21-AGAM-presentation.pdf">slide</a>] <a href="https://scholar.google.com/citations?view_op=view_citation&amp;citation_for_view=mhpkWSYAAAAJ:9yKSN-GCB0IC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&amp;query=publications.2.citations&amp;url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&amp;logo=googlescholar&amp;style=social" /></a> <a href="https://github.com/bighuang624/AGAM" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/AGAM?style=social" /></a></p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" target="_blank"><img src="https://img.shields.io/badge/CIKM-2019-blue?style=flat-square" /></a> <u>Siteng Huang</u>, Donglin Wang, Xuehan Wu, Ao Tang, "<strong>DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting</strong>". In <em>Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>. [<a href="https://kyonhuang.top/publication/dual-self-attention-network">project page</a>] [<a href="https://kyonhuang.top/files/DSANet/Huang-DSANet.pdf">pdf</a>] [<a href="https://github.com/bighuang624/DSANet">code</a>] [<a href="https://kyonhuang.top/files/DSANet/cikm19-DSANet-poster.pdf">poster</a>] [<a href="https://kyonhuang.top/files/DSANet/cikm19-DSANet-presentation.pdf">slide</a>] <a href="https://scholar.google.com/citations?view_op=view_citation&amp;citation_for_view=mhpkWSYAAAAJ:u-x6o8ySG0sC" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=citations&amp;query=publications.0.citations&amp;url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&amp;logo=googlescholar&amp;style=social" /></a> <a href="https://github.com/bighuang624/DSANet" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624/DSANet?style=social" /></a></p>

<!-- <a href="https://dl.acm.org/doi/abs/10.1145/3357384.3358132" style="text-decoration:none;"><span style="font-size:12px;color:#FFFFFF;background-color:#555555;padding:1px 4px 2px 6px;">CIKM</span><span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 6px 2px 4px;">2019</span></a> -->

<h3 id="preprints--under-submission">Preprints &amp; Under Submission</h3>

<p><a href="https://arxiv.org/abs/2309.01141" target="_blank"><img src="https://img.shields.io/badge/arXiv-2309.01141-B31B1B?style=flat-square" /></a> Xuyang Liu†, <u>Siteng Huang</u>†, Yachen Kang, Honggang Chen, Donglin Wang, "<strong>VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders</strong>". <em>arXiv preprint arXiv:2309.01141</em>. [<a href="https://arxiv.org/pdf/2309.01141.pdf">pdf</a>]</p>

<p><a href="https://arxiv.org/abs/2303.15230" target="_blank"><img src="https://img.shields.io/badge/arXiv-2303.15230-B31B1B?style=flat-square" /></a> <u>Siteng Huang</u>, Biao Gong, Yutong Feng, Yiliang Lv, Donglin Wang, "<strong>Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning</strong>". <em>arXiv preprint arXiv:2303.15230</em>. [<a href="https://arxiv.org/pdf/2303.15230.pdf">pdf</a>] [<a href="https://kyonhuang.top/publication/Troika">project page</a>] [<a href="https://github.com/bighuang624/Troika">github</a>]</p>

<!-- ## Professional Experience -->

<h2 id="experience">Experience</h2>

<ul>
  <li>Research Intern - <strong>DAMO Academy, Alibaba Group</strong>
    <ul>
      <li>Time: March 2022 - Present.</li>
      <li>Fundamental Visual Intelligence Team.</li>
    </ul>
  </li>
</ul>

<!-- **Research Intern** | DAMO Academy, Alibaba Group | March 2022 - Present -->

<!-- * March 2022 - Present. *Research Intern*. <a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a>, Hangzhou, China. -->

<!-- <div style="float:left;">Research Intern</div><div style="float:right;">Mar. 2022 - Present</div>
<div style="float:left;"><a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a></div><div style="float:right;">Hangzhou, China</div> -->

<!-- <div>
<div style="float:left;">Research Intern<br><a href="https://damo.alibaba.com/" target="_blank">DAMO Academy, Alibaba Group</a></div><div style="float:right;">Mar. 2022 - Present<br>Hangzhou, China</div>
</div> -->

<h2 id="services">Services</h2>

<!-- ### Journal Reviewer

* [IEEE Transactions on Neural Networks and Learning Systems (TNNLS)](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385)

### Program Committee and/or Reviewer for Conferences and Workshops

* [ICCV 2023](https://iccv2023.thecvf.com/) -->

<h3 id="conference-and-journal-reviewer">Conference and Journal Reviewer</h3>

<ul>
  <li>IEEE Transactions on Neural Networks and Learning Systems <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">(TNNLS)</a></li>
  <li>IEEE/CVF International Conference on Computer Vision <a href="https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings">(ICCV)</a></li>
  <li>AAAI Conference on Artificial Intelligence <a href="https://aaai.org/conference/aaai/">(AAAI)</a></li>
  <li>International Joint Conference on Artificial Intelligence <a href="https://www.ijcai.org/">(IJCAI)</a></li>
  <li>European Conference on Computer Vision <a href="https://www.ecva.net/index.php#conferences">(ECCV)</a></li>
</ul>

<h3 id="program-committee-for-conferences-and-workshops">Program Committee for Conferences and Workshops</h3>

<ul>
  <li>Session Chair, The First Westlake Robot Learning Symposium</li>
</ul>

<h2 id="misc">Misc</h2>

<p>Welcome to follow my <a href="https://www.zhihu.com/people/huang-si-teng-67">Zhihu</a> account and <a href="https://kyonhuang.top/blog/">Chinese blog</a>.</p>

<!-- <div align="middle">
  <a href="https://milab.westlake.edu.cn/" target="_blank"><img align="middle" style="max-width: 300px; width: 100%; margin-right: 40px; margin-top: 10px" src="https://kyonhuang.top/images/milab_logo.png" /></a>
  <a href="http://www.zju.edu.cn/" target="_blank"><img align="middle" style="max-width: 160px; width: 100%; margin-left: 20px; margin-top: 10px" src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/color-zju-logo.png" /></a>
</div> -->

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- moved to footer.html -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<a href="/sitemap/">Sitemap</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span> views.</span>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/bighuang624"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>



<div class="page__footer-copyright">&copy; 2023 Siteng Huang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

