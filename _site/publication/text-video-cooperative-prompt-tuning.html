

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval - Yalun Wu</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yalun Wu">
<meta property="og:title" content="VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval">


  <link rel="canonical" href="http://localhost:4000/publication/text-video-cooperative-prompt-tuning">
  <meta property="og:url" content="http://localhost:4000/publication/text-video-cooperative-prompt-tuning">



  <meta property="og:description" content="In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video &amp; text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full finetuning, the enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval benchmarks with 6× less parameter overhead.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2023-05-01T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yalun Wu",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Yalun Wu Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Yalun Wu</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#news">News</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#publications">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#Projects">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000\#services">Services</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.jpg" class="author__avatar" alt="Yalun Wu">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yalun Wu</h3>
    <p class="author__bio">Ph.D. Student @ Beijing Jiaotong University</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China</li>
      
      
      
      
        <li><a href="mailto:wuyalun1@bjtu.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/Allenpandas"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=BF5dco0AAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval">
    <meta itemprop="description" content="In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video &amp; text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full finetuning, the enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval benchmarks with 6× less parameter overhead.">
    <meta itemprop="datePublished" content="May 01, 2023">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval
</h1>
          
        
        
        
          <!-- <p>Published in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (CVPR 2023)</i>, 2023 </p> -->
          <p>Published in <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (CVPR 2023)</i></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p><a href="https://arxiv.org/abs/2211.12764" class="btn btn--info">arXiv</a>
<a href="https://github.com/bighuang624/VoP" class="btn btn--info">github</a>
<a href="https://www.youtube.com/watch?v=ymdkiSSuOmI" class="btn btn--info">video (Youtube)</a>
<a href="https://kyonhuang.top/files/VoP/CVPR23-VoP-poster.pdf" class="btn btn--info">poster</a>
<a href="https://kyonhuang.top/files/VoP/CVPR23-VoP-presentation.pdf" class="btn btn--info">slide</a>
<a href="https://modelscope.cn/models/damo/cv_vit-b32_retrieval_vop/summary" class="btn btn--info">ModelScope</a></p>

<!-- ![](https://img.shields.io/badge/arXiv-2211.12764-B31B1B?style=flat) -->

<h2 id="background">Background</h2>

<div align="middle"><img align="middle" style="max-width: 560px; width: 100%" src="https://kyonhuang.top/files/VoP/VoP-comparison.png" /></div>

<!-- In this paper, we introduce prompt tuning to address the challenges that limit the transferability and generalizability. Keeping the backbone frozen and only tuning a few extra parameters prepended to the input, prompt tuning has been widely applied as a flexible and light-weight fine-tuning protocol. Compared to uni-modal applications, text-video cross-modal retrieval requires more parameters to support the dualbranch structure, making it logical to benefit from the parameter-efficient tuning strategy. In addition, different from text descriptions that compose sequential information from words, video-understanding requires summarizing information in both the spatial and temporal dimensions. Therefore, we assume that designing nontrivial video prompts further contributes to prompting both branches for mutual promotion. -->
<p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this paper, we continue the vein of prompt tuning to transfer pre-trained CLIP for text-video retrieval with both effectiveness and efficiency. Main contributions are:
<!-- We first devise a simple but competitive baseline VoP, which achieves promising performance with only 0.1% trainable parameters by prompting all layers of both textual and visual encoders. To increase the revenue of VoP, we further explore three video prompts to model different video-specific information. Different combinations of our video prompts can be selected depending on the strictness of the limits on parameter overhead, and achieve at most 1.4% average relative improvement with much fewer trainable parameters compared to full fine-tuning.  --></p>

<ol>
  <li>
    <p>We propose the VoP as a strong baseline that effectively adapts CLIP to text-video retrieval with only 0.1% trainable parameters.</p>
  </li>
  <li>
    <p>To exploit video-specific information, we further develop three video prompts respectively conditioned on the frame position, frame context, and layer function. To the best of our knowledge, this is the first work that explores video-specific prompts.</p>
  </li>
  <li>
    <p>Extensive experiments on five text-video retrieval benchmarks demonstrate that various combinations of our video prompts effectively enhance VoP, achieving at most 1.4% average relative improvement with much fewer trainable parameters compared to full fine-tuning.</p>
  </li>
</ol>

<h2 id="model-overview">Model Overview</h2>

<p><img src="https://kyonhuang.top/files/VoP/VoP-overview.png" alt="" /></p>

<p>We propose the <strong>VoP</strong>: Text-<strong>V</strong>ideo C<strong>o</strong>-operative <strong>P</strong>rompt Tuning to simultaneously introduce tunable prompts in both textual and visual encoders. Also, different from existing related efforts that only insert prompt vectors into the input textual sequences, we find that preparing prompts for every layer of both encoders can further close the gap to full fine-tuning. To exploit essential video-specific information, we further design three novel video prompts from different perspectives, which can seamlessly replace conventional visual prompts in VoP. Specifically,</p>

<ol>
  <li><strong>position-specific</strong> video prompts model the information shared between frames at the same relative position.</li>
  <li>Generated <strong>context-specific</strong> video prompts integrate injected contextual message from the frame sequence into the intra-frame modeling.</li>
  <li>And <strong>function-specific</strong> video prompts adaptively assist to learn intra- or inter-frame affinities by sensing the transformation of layer functions.</li>
</ol>

<p>By exploring video-specific prompts, VoP offers a new way to transfer pre-trained foundation models to the downstream video domain.</p>

<h2 id="experiment-results">Experiment Results</h2>

<p>Here we report some experimental results to empirically show the effectiveness and efficiency of our VoP series. Please check the paper for the details of the experiment settings and further analysis.</p>

<h3 id="main-results">Main Results</h3>

<p>The following results are obtained with a pre-trained CLIP (ViT-B/32). More experimental results can be found in the paper.</p>

<p><em>t2v</em> and <em>v2t</em> retrieval results on MSR-VTT-9k dataset:</p>

<p><img src="https://kyonhuang.top/files/VoP/VoP-MSRVTT9k-results.png" alt="" /></p>

<p><em>t2v</em> relative results on all datasets:</p>

<div align="middle"><img align="middle" style="max-width: 560px; width: 100%" src="https://kyonhuang.top/files/VoP/VoP-t2v-results.png" /></div>

<h3 id="qualitative-results">Qualitative Results</h3>

<p><img src="https://kyonhuang.top/files/VoP/VoP-qualitative-results.png" alt="" /></p>

<!-- We represent the retrieval results of four tuning methods: Full, Partial, VoP, and VoP<sup>F+C</sup>.  -->

<ul>
  <li>In the <strong>top left</strong> example, Full and our proposed methods can retrieve the correct video while Partial matches an unrelated one, which shows the inferiority of existing efficient tuning protocols.</li>
  <li>In the <strong>top right</strong> example, Full fails to recognize a “Japanese” book while parameter-efficient tuning methods succeed by capturing visual clues of Japanese characters and related English words like “Tokyo”, indicating that updating all parameters might be an unsatisfactory strategy as more knowledge from large-scale text-image pre-training is forgotten.</li>
  <li>In the <strong>bottom left</strong> example, by fine-tuning all parameters with video datasets or designing specialized prompting solutions for videos, Full and VoP<sup>F+C</sup> can understand the whole event represented by sequenced frames. Even if some textual elements like “priest” are not visually present, the methods overcome such minor semantic misalignments and select more relevant candidates from a global view.</li>
  <li>In the <strong>bottom right</strong> example, understanding the concept of “tail” and capturing the interaction of “playing with”, VoP<sup>F+C</sup> can distinguish the correct video from hard negative candidates while all the other three methods fail.</li>
</ul>

<h2 id="bibtex">BibTex</h2>

<p>If you find this work useful in your research, please cite our paper:</p>

<pre>
@inproceedings{Huang2023VoP,
    title={VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval},
    author={Siteng Huang and Biao Gong and Yulin Pan and Jianwen Jiang and Yiliang Lv and Yuyuan Li and Donglin Wang},
    booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition 2023},
    month = {June},
    year = {2023}
}
</pre>

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/publication/text-video-cooperative-prompt-tuning" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/publication/text-video-cooperative-prompt-tuning" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/publication/text-video-cooperative-prompt-tuning" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="http://localhost:4000/publication/Troika" class="pagination--pager" title="Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- moved to footer.html -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<a href="/sitemap/">Sitemap</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span> views.</span>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/Allenpandas"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>



<div class="page__footer-copyright">&copy; 2023 Yalun Wu. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

